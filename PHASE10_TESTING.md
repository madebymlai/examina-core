# Phase 10: Learning Materials - Testing Guide

## Status: Testing Blocked - API Keys Required

**Last Updated:** 2025-11-24

---

## ðŸ› Bug Fixes Applied (Commit d2314c2)

### 1. Missing `clean_exercise_text()` Method
- **Issue:** `SmartExerciseSplitter` lacked this method, causing `AttributeError` during ingestion
- **Fix:** Added delegation to `ExerciseSplitter.clean_exercise_text()`
- **File:** `core/smart_splitter.py:549-560`

### 2. Notes Mode Not Processing All Pages
- **Issue:** LLM only processed pages WITHOUT pattern-based exercises, missing theory/worked examples on exercise pages
- **Fix:** Added `notes_mode` parameter - when `True`, processes ALL pages with LLM
- **File:** `core/smart_splitter.py:70` (parameter), `core/smart_splitter.py:140` (logic)
- **File:** `cli.py:384` (CLI passes `notes_mode=True`)

### 3. Improved Error Logging
- **Issue:** Silent LLM failures showed only "Failed to parse JSON" without context
- **Fix:** Added checks for `response.success` and empty responses with clear error messages
- **File:** `core/smart_splitter.py:225-231`

---

## âš ï¸ Current Blocker: Invalid API Keys

### Issue Details

Testing is blocked because API keys are invalid:

```
Error: Invalid ANTHROPIC_API_KEY. Check your API key.
âš ï¸  LLM returned empty response for page X
âš ï¸  Failed to parse LLM response as JSON: Expecting value: line 1 column 1 (char 0)
```

**Root Cause:**
- `ANTHROPIC_API_KEY` in `.env` is invalid/expired
- `GROQ_API_KEY` is not set

---

## ðŸ”§ Fix: Update API Keys

### Option 1: Use Anthropic (Recommended for Quality)

```bash
# Get a new key from https://console.anthropic.com/settings/keys

# Edit .env file
nano /home/laimk/git/Examina/.env

# Update or add:
ANTHROPIC_API_KEY=sk-ant-api03-YOUR_VALID_KEY_HERE
```

### Option 2: Use Groq (Free Tier, Fast)

```bash
# Get a free key from https://console.groq.com

# Edit .env file
nano /home/laimk/git/Examina/.env

# Add:
GROQ_API_KEY=gsk_YOUR_GROQ_KEY_HERE
```

### Option 3: Use Ollama (Local, No API Key Needed)

```bash
# Install and start Ollama
curl https://ollama.ai/install.sh | sh
ollama serve

# Pull a model
ollama pull llama3.2:3b

# Test with Ollama (no API key required)
python3 cli.py ingest --course PHASE10_TEST \
  --zip /home/laimk/Downloads/TEST-PHASE-10/Appunti-AE.zip \
  --material-type notes \
  --provider ollama
```

---

## ðŸ“‹ Testing Checklist (Once API Keys Are Valid)

### 1. Ingest Lecture Notes

```bash
# Clear previous test data
python3 -c "
from storage.database import Database
with Database() as db:
    db.conn.execute('DELETE FROM exercises WHERE course_code = \"PHASE10_TEST\"')
    db.conn.execute('DELETE FROM learning_materials WHERE course_code = \"PHASE10_TEST\"')
    db.conn.commit()
    print('âœ… Test data cleared')
"

# Ingest with notes mode
python3 cli.py ingest --course PHASE10_TEST \
  --zip /home/laimk/Downloads/TEST-PHASE-10/Appunti-AE.zip \
  --material-type notes \
  --provider anthropic  # or groq or ollama
```

**Expected Output:**
```
ðŸ“š Processing lecture notes with smart content detection (anthropic)
   Detecting theory sections, worked examples, and practice exercises
...
âœ“ Found X exercise(s) (pattern: Y, LLM: Z)
âœ“ Found A theory section(s), B worked example(s)
LLM processed 23/23 pages (est. cost: $X.XXXX)
```

### 2. Verify Extraction

```bash
python3 -c "
from storage.database import Database

with Database() as db:
    exercises = db.conn.execute('SELECT COUNT(*) FROM exercises WHERE course_code = \"PHASE10_TEST\"').fetchone()[0]
    materials = db.conn.execute('SELECT COUNT(*) FROM learning_materials WHERE course_code = \"PHASE10_TEST\"').fetchone()[0]
    theory = db.conn.execute('SELECT COUNT(*) FROM learning_materials WHERE course_code = \"PHASE10_TEST\" AND material_type = \"theory\"').fetchone()[0]
    examples = db.conn.execute('SELECT COUNT(*) FROM learning_materials WHERE course_code = \"PHASE10_TEST\" AND material_type = \"worked_example\"').fetchone()[0]

    print(f'âœ… Exercises: {exercises}')
    print(f'ðŸ“š Learning Materials: {materials}')
    print(f'   - Theory sections: {theory}')
    print(f'   - Worked examples: {examples}')
"
```

**Success Criteria:**
- âœ… `materials > 0` (learning materials extracted)
- âœ… `theory > 0` (theory sections detected)
- âœ… Coverage: theory ~70%+, worked examples ~60%+ of actual content

### 3. Analyze and Link Materials

```bash
# Analyze course to detect topics
python3 cli.py analyze --course PHASE10_TEST --force --provider anthropic

# Link materials to topics
python3 cli.py link-materials --course PHASE10_TEST
```

**Expected Output:**
```
[ANALYSIS] Analyzing 23 exercises...
[ANALYSIS] Analyzing X learning materials...
[LINK] Material â†’ Topic "..." (similarity: 0.XX)
[LINK] Example â†’ Exercise "..." (similarity: 0.XX)
```

### 4. Test Learning Flow

```bash
# List topics
python3 cli.py topics --course PHASE10_TEST

# Test learn command with a topic
python3 cli.py learn --course PHASE10_TEST --topic "TOPIC_NAME"
```

**Expected Output:**
```
THEORY MATERIALS
Before starting with exercises, let's review the foundational theory:

## [Theory Title]
[Source: Appunti-AE-1-semestre.pdf, page X]
[Theory content...]

===

WORKED EXAMPLES
Now let's see how to apply this theory through step-by-step worked examples:

### [Example Title]
[Source: Appunti-AE-1-semestre.pdf, page Y]
[Example content...]

===

[LLM-generated explanation of the core loop...]
```

### 5. Verify Configuration

```bash
# Check that Config values are being used
python3 -c "
from config import Config
print(f'SHOW_THEORY_BY_DEFAULT: {Config.SHOW_THEORY_BY_DEFAULT}')
print(f'MAX_THEORY_SECTIONS_IN_LEARN: {Config.MAX_THEORY_SECTIONS_IN_LEARN}')
print(f'MAX_WORKED_EXAMPLES_IN_LEARN: {Config.MAX_WORKED_EXAMPLES_IN_LEARN}')
print(f'WORKED_EXAMPLE_EXERCISE_SIMILARITY_THRESHOLD: {Config.WORKED_EXAMPLE_EXERCISE_SIMILARITY_THRESHOLD}')
"
```

---

## ðŸŽ¯ Success Criteria

### Design Principles (All Verified in Code)
- âœ… Smart splitter acts as classifier (returns SplitResult)
- âœ… Ingestion modes describe document type (--material-type flag)
- âœ… Topic linking treats materials symmetrically
- âœ… Tutor flow explicit and configurable
- âœ… No regression in exam pipeline

### Functional Requirements (Need Testing)
- [ ] Pattern-based splitting works for exams
- [ ] Notes ingestion creates theory and worked examples
- [ ] Materials link to multiple topics
- [ ] Worked examples link to exercises
- [ ] Tutor shows theory â†’ examples â†’ practice

### Configuration (All Implemented)
- âœ… All thresholds in Config
- âœ… Provider-agnostic
- âœ… Bilingual support
- âœ… Web-ready design

### Coverage Goals
- [ ] Theory sections: 70%+ detection rate
- [ ] Worked examples: 60%+ detection rate
- [ ] False positives: <10% error rate
- [ ] No regression on exam PDFs

---

## ðŸ“ Test Results

### âœ… Test Run 1: 2025-11-24 - SUCCESS
- **Provider:** groq (llama-3.3-70b-versatile)
- **PDF:** Appunti-AE-3pages.pdf (3 pages - Italian lecture notes)
- **Exercises Extracted:** 3 (pattern-based detection)
- **Learning Materials:** 20 total (19 theory, 1 worked example)
- **Topics Detected:** 1 ("Performance Metrics and Evaluation")
- **Material-Topic Links:** 4 (semantic similarity matching 0.90-0.96)
- **Example-Exercise Links:** 0 (worked example on different topic)
- **LLM Processing:** 3/3 pages processed with cache hits
- **Tutor Flow:** âœ… Theory â†’ LLM Explanation working perfectly

**Issues Encountered:**
1. Initial Anthropic API key invalid - switched to Groq
2. CLI and Tutor hardcoded to use Anthropic provider - fixed to use Config.LLM_PROVIDER
3. Rate limiting kicked in during material linking (~56s waits) - handled gracefully

**Key Observations:**
- Theory materials correctly extracted and displayed in learn command
- Semantic matching working well (detected "Computer Performance Metrics" â†’ "Performance Metrics and Evaluation")
- Many unmatched topics expected (materials cover broader content than exercises)
- Cache system working perfectly (3/3 cache hits on re-processing)
- Italian content handled correctly
- Theory â†’ Practice flow demonstrates Phase 10 design principles successfully

---

## ðŸ” Debugging Tips

### If No Materials Are Extracted

Check LLM responses:
```bash
tail -50 /tmp/phase10_test.log | grep "âš ï¸"
```

### If Materials But No Links

```bash
python3 -c "
from storage.database import Database
with Database() as db:
    links = db.conn.execute('SELECT COUNT(*) FROM material_topics').fetchone()[0]
    print(f'Material-Topic Links: {links}')
"
```

### Test Single Page Classification

```python
from core.smart_splitter import SmartExerciseSplitter
from models.llm_manager import LLMManager
from core.pdf_processor import PDFProcessor

llm = LLMManager(provider='anthropic')
splitter = SmartExerciseSplitter(llm_manager=llm, notes_mode=True)

# Test with sample text
test_text = "Your test content here..."
prompt = splitter._build_detection_prompt(test_text)
print(prompt)

response = llm.generate(prompt=prompt, temperature=0.0, max_tokens=1000)
print(f"Success: {response.success}")
print(f"Response: {response.text[:500]}")
```

---

## ðŸ“š Related Documentation

- [PHASE10_DESIGN.md](PHASE10_DESIGN.md) - Design principles and contracts
- [PHASE10_IMPLEMENTATION_REVIEW.md](PHASE10_IMPLEMENTATION_REVIEW.md) - Implementation review
- [TODO.md](TODO.md) - Phase 10 status and next steps
